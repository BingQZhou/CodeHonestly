{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 158 HW 3\n",
    "#### 11/12/19\n",
    "#### Hongtao Jiang A13760857\n",
    "#### Kaggle User Name: Hongtao Jiang (Parker)\n",
    "    Note: It shows as Hongtao Jiang on the leaderboard but the one in my profile is Parker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before answering the questions, I imported all the models that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "from sklearn import metrics\n",
    "import gzip\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported the two methods from the Baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)\n",
    "\n",
    "def readCSV(path):\n",
    "    f = pd.read_csv(path)\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I read the data and split it into training set and validation set. While I created two sets, I added a binary column to show if the reader has read the book. And then, I add 10000 negative entries to the validation sets. And save training set and validation set into two txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "f = gzip.open('train_Interactions.csv.gz','rt',encoding = 'utf8')\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "\n",
    "# generate training set\n",
    "train_data = []\n",
    "for i in range(0,190000):\n",
    "    l = f.readline().strip().split(',')\n",
    "    train_data.append(l)\n",
    "    \n",
    "# generate validation set  \n",
    "valid_data = []\n",
    "for i in range(190001,200001):\n",
    "    l = f.readline().strip().split(',')\n",
    "    l.append('1')\n",
    "    valid_data.append(l)\n",
    "\n",
    "total_data = train_data+valid_data\n",
    "\n",
    "# save training set to train_data.txt\n",
    "with open('train_data.txt', 'w') as f:\n",
    "    for i in train_data:\n",
    "        f.write(i[0] +',' + i[1] +',' + i[2] + ',' + '1' + '\\n')\n",
    "\n",
    "# create a dictionary with key as users and \n",
    "# values as the books that the user has read        \n",
    "bookUser_valid = defaultdict(list)\n",
    "for i in valid_data:\n",
    "    bookUser_valid[i[0]].append(i[1])\n",
    "\n",
    "# create a list that contains all the books\n",
    "book_lst = []\n",
    "for i in total_data:\n",
    "    book_lst.append(i[1])   \n",
    "book_lst1 = list(set(book_lst))\n",
    "\n",
    "user_lst = []\n",
    "for i in total_data:\n",
    "    user_lst.append(i[0])   \n",
    "user_lst1 = list(set(user_lst))\n",
    "\n",
    "# generate 10000 random negative entries for the validation set\n",
    "sample = []\n",
    "for i in valid_data:\n",
    "    random_book = random.choice(book_lst1)\n",
    "    while random_book in bookUser_valid[i[0]]:\n",
    "        random_book = random.choice(book_lst1)\n",
    "    sample.append([i[0],random_book,'0','0'])\n",
    "    \n",
    "valid_data = valid_data + sample\n",
    "\n",
    "# save validation set to valid_data.txt\n",
    "with open('valid_data.txt', 'w') as f:\n",
    "    for i in valid_data:\n",
    "        f.write(i[0] +',' + i[1] +',' + i[2] + ',' + i[3] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is modified baseline model from the baseline.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.643\n"
     ]
    }
   ],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for l in open('train_data.txt'):\n",
    "    u,book,r,i = l.strip().split(',') \n",
    "    bookCount[book]+=1\n",
    "    totalRead+=1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break\n",
    "\n",
    "# create a correct variable to calculate the accuracy later        \n",
    "correct = 0\n",
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for l in open(\"valid_data.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b,r,i = l.strip().split(',')\n",
    "    if b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "        if i == '1':\n",
    "            correct += 1\n",
    "            \n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "        if i == '0':\n",
    "            correct += 1\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "# calculate the accuracy score for baseline model\n",
    "print('Accuracy Score:',correct/len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To locate the threshold with the highest accuracy, I run the model multiple times with different threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score with 0.1 threshold: 0.53975\n",
      "Accuracy Score with 0.2 threshold: 0.5745\n",
      "Accuracy Score with 0.3 threshold: 0.6072\n",
      "Accuracy Score with 0.4 threshold: 0.6289\n",
      "Accuracy Score with 0.5 threshold: 0.643\n",
      "Accuracy Score with 0.566 threshold: 0.64885\n",
      "Accuracy Score with 0.7 threshold: 0.641\n",
      "Accuracy Score with 0.8 threshold: 0.617\n",
      "Accuracy Score with 0.9 threshold: 0.578\n"
     ]
    }
   ],
   "source": [
    "# create a list of threshold to test which one has the highest accuract\n",
    "threshold = [0.1,0.2,0.3,0.4,0.5,0.566,0.7,0.8,0.9]\n",
    "\n",
    "for j in threshold:\n",
    "\n",
    "    return2 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return2.add(i)\n",
    "        if count > totalRead*j: break\n",
    "\n",
    "        \n",
    "    correct = 0\n",
    "    predictions = open(\"predictions_Read.txt\", 'w')\n",
    "    for l in open(\"valid_data.txt\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            #header\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        u,b,r,i = l.strip().split(',')\n",
    "        if b in return2:\n",
    "            predictions.write(u + '-' + b + \",1\\n\")\n",
    "            if i == '1':\n",
    "                correct += 1\n",
    "            \n",
    "        else:\n",
    "            predictions.write(u + '-' + b + \",0\\n\")\n",
    "            if i == '0':\n",
    "                correct += 1\n",
    "\n",
    "    predictions.close()\n",
    "\n",
    "    print('Accuracy Score with ' + str(j) + ' threshold:',correct/len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that when the threshold is around 0.566, the accuracy is the highest, which is 0.64855."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calculate the Jaccard Similarity, I created two dictionary to avoid including too many loops in my method. The first dictionary's key is user and the values are the books that the user has read. The second dictionary's key is book and values are the users that have read the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookUser_train = defaultdict(list)\n",
    "for i in train_data:\n",
    "    bookUser_train[i[0]].append(i[1])\n",
    "    \n",
    "userBook_train = defaultdict(list)\n",
    "for i in train_data:\n",
    "    userBook_train[i[1]].append(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose the threshold to be 0.01 so that the accuracy is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard-User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define jaccard similarity function\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "max_distance_lst = []\n",
    "output_lst = []\n",
    "for i in valid_data:\n",
    "    # book that the user in the pair from validation set has read\n",
    "    b_user = bookUser_train[i[0]]\n",
    "    # users that have read the book in the pair in train data\n",
    "    if i[1] in userBook_train:\n",
    "        u_book = userBook_train[i[1]]\n",
    "    else:\n",
    "        u_book = []\n",
    "    # for every book that is not the book in the pair, get all users for each book \n",
    "    u_book_xb = []\n",
    "    for book in b_user:\n",
    "        if book in userBook_train:\n",
    "            u_book_xb += [userBook_train[book]]\n",
    "    scores = [0]\n",
    "    # compute the jaccard similarity list\n",
    "    for user in u_book_xb:\n",
    "        scores.append(jaccard_similarity(user,u_book))\n",
    "    # if the maximum of the list exceeds the threshold, predict positive\n",
    "    max_distance_lst.append(max(scores))\n",
    "    if max(scores) > 0.01:\n",
    "        output_lst.append('1')\n",
    "    else:\n",
    "        output_lst.append('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score from Jaccard-User Similarity with a Threshold of 0.01: 0.6202\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for i in valid_data:\n",
    "    y.append(i[3])\n",
    "print('Accuracy Score from Jaccard-User Similarity with a Threshold of 0.01:', metrics.accuracy_score(y, output_lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard-Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distance_lst_bu = []\n",
    "output_lst_bu = []\n",
    "for i in valid_data:\n",
    "    # users that the book in the pair from validation set has been read by them\n",
    "    user_b = userBook_train[i[1]]\n",
    "    # books that have been read by the user in the pair in train data\n",
    "    if i[0] in bookUser_train:\n",
    "        b_user = bookUser_train[i[0]]\n",
    "    else:\n",
    "        b_user = []\n",
    "    # for every user that is not the user in the pair, get all books for each user \n",
    "    b_user_xb = []\n",
    "    for user in user_b:\n",
    "        if user in bookUser_train:\n",
    "            b_user_xb += [bookUser_train[user]]\n",
    "    scores_bu = [0]\n",
    "    # compute the jaccard similarity list\n",
    "    for book in b_user_xb:\n",
    "        scores_bu.append(jaccard_similarity(book,b_user))\n",
    "    # if the maximum of the list exceeds the threshold, predict positive\n",
    "    max_distance_lst_bu.append(max(scores_bu))\n",
    "    if max(scores_bu) > 0.03:\n",
    "        output_lst_bu.append('1')\n",
    "    else:\n",
    "        output_lst_bu.append('0')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score from Jaccard-Book Similarity with a Threshold of 0.03: 0.6385\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for i in valid_data:\n",
    "    y.append(i[3])\n",
    "print('Accuracy Score from Jaccard-Book Similarity with a Threshold of 0.03:', metrics.accuracy_score(y, output_lst_bu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine two models, I add the jaccard similarity as a condition that the book must not only be among the popular ranking above 0.0566 threshold, but also has a jaccard similarity outcome of 1 (positive label) to be predicted as read by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for the combined model: 0.6499\n"
     ]
    }
   ],
   "source": [
    "if_popular = []\n",
    "\n",
    "return4 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return4.add(i)\n",
    "    if count > totalRead*0.566: break\n",
    "        \n",
    "for i in valid_data:\n",
    "    if i[1] in return4:\n",
    "        if_popular.append(1)\n",
    "    else:\n",
    "        if_popular.append(0)\n",
    "\n",
    "index = 0\n",
    "correct = 0\n",
    "for l in open(\"valid_data.txt\"):\n",
    "    u,b,r,i = l.strip().split(',')\n",
    "    # add the output list from jaccard similarity as a condition\n",
    "    if b in return4 and output_lst[index] == '1':\n",
    "        if i == '1':\n",
    "            correct += 1            \n",
    "    else:\n",
    "        if i == '0':\n",
    "            correct += 1\n",
    "    index += 1\n",
    "\n",
    "\n",
    "print('Accuracy Score for the combined model:',correct/len(valid_data))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle User Name: Hongtao Jiang (Parker)\n",
    "    Note: It shows as Hongtao Jiang on the leaderboard but the one in my profile is Parker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model on testing set, we need to re-train out jaccard similarity model so that it will perform well on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_data = []\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    test_data.append([u,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I defined two functions to compute jaccard similarity. \n",
    "\n",
    "The first one is *compute_jaccard_max*, which returns the binary based on if the maximum of each similarity list exceed the threshold. \n",
    "\n",
    "The second one is *compute_jaccard_mean*, which returns the binary based on if the mean of each similarity list exceed the threshold.\n",
    "\n",
    "Then, to combine the popularity and the jaccard similarity, I first check if the book in the pair is in the popularity rank above the threshold, then if it is, I set an index to 1. Then I sum the index with three different jaccard similarity computation (First two with compute_jaccard_max with different threshold parameter, and the last one with compute_jaccard_mean) to see if the sum exceeds 2. If it does, I predict the outcome to be positive. Otherwise, the prediction is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(a,b):\n",
    "    a=set(a)\n",
    "    b=set(b)\n",
    "    numer = len(a.intersection(b))\n",
    "    if len(a)==0 or len(b)==0:\n",
    "        return 0\n",
    "    cos = numer /(pow(len(a),1/2)*pow(len(b),1/2))\n",
    "    return cos\n",
    "\n",
    "def compute_cosine_max(data,threshhold=0.02):\n",
    "    output_lst = []\n",
    "    for i in data:\n",
    "        # book that the user in the pair from validation set has read\n",
    "        b_user = bookUser_train[i[0]]\n",
    "        # users that have read the book in the pair in train data\n",
    "        if i[1] in userBook_train:\n",
    "            u_book = userBook_train[i[1]]\n",
    "        else:\n",
    "            u_book = []\n",
    "        # for every book that is not the book in the pair, get all users for each book \n",
    "        u_book_xb = []\n",
    "        for book in b_user:\n",
    "            if book in userBook_train:\n",
    "                u_book_xb += [userBook_train[book]]\n",
    "        scores = [0]\n",
    "        # compute\n",
    "        for user in u_book_xb:\n",
    "            scores.append(cos_similarity(user,u_book))\n",
    "        if max(scores) > threshhold:\n",
    "            output_lst.append(1)\n",
    "        else:\n",
    "            output_lst.append(0)\n",
    "    return output_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59975\n"
     ]
    }
   ],
   "source": [
    "l=compute_cosine_max(valid_data, threshhold=0.025)\n",
    "y = []\n",
    "acc=0\n",
    "for i in valid_data:\n",
    "    y.append(i[3])\n",
    "for i in range(len(y)):\n",
    "    #print(\"i\")\n",
    "    if int(y[i])==l[i]:\n",
    "        acc+=1\n",
    "print(acc/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_max(threshhold=0.01):\n",
    "    output_lst = []\n",
    "    for i in test_data:\n",
    "        # book that the user in the pair from validation set has read\n",
    "        b_user = bookUser_train[i[0]]\n",
    "        # users that have read the book in the pair in train data\n",
    "        if i[1] in userBook_train:\n",
    "            u_book = userBook_train[i[1]]\n",
    "        else:\n",
    "            u_book = []\n",
    "        # for every book that is not the book in the pair, get all users for each book \n",
    "        u_book_xb = []\n",
    "        for book in b_user:\n",
    "            if book in userBook_train:\n",
    "                u_book_xb += [userBook_train[book]]\n",
    "        scores = [0]\n",
    "        # compute\n",
    "        for user in u_book_xb:\n",
    "            scores.append(jaccard_similarity(user,u_book))\n",
    "        if max(scores) > threshhold:\n",
    "            output_lst.append(1)\n",
    "        else:\n",
    "            output_lst.append(0)\n",
    "    return output_lst\n",
    "\n",
    "def compute_jaccard_max_2(threshhold=0.03):\n",
    "    output_lst_bu = []\n",
    "    for i in test_data:\n",
    "        # users that the book in the pair from validation set has been read by them\n",
    "        user_b = userBook_train[i[1]]\n",
    "        # books that have been read by the user in the pair in train data\n",
    "        if i[0] in bookUser_train:\n",
    "            b_user = bookUser_train[i[0]]\n",
    "        else:\n",
    "            b_user = []\n",
    "        # for every user that is not the user in the pair, get all books for each user \n",
    "        b_user_xb = []\n",
    "        for user in user_b:\n",
    "            if user in bookUser_train:\n",
    "                b_user_xb += [bookUser_train[user]]\n",
    "        scores_bu = [0]\n",
    "        # compute the jaccard similarity list\n",
    "        for book in b_user_xb:\n",
    "            scores_bu.append(jaccard_similarity(book,b_user))\n",
    "        # if the maximum of the list exceeds the threshold, predict positive\n",
    "        if max(scores_bu) > threshhold:\n",
    "            output_lst_bu.append(1)\n",
    "        else:\n",
    "            output_lst_bu.append(0)\n",
    "    return output_lst_bu\n",
    "        \n",
    "\n",
    "def compute_jaccard_mean(threshhold=0.001):\n",
    "    output_lst = []\n",
    "    for i in test_data:\n",
    "        # book that the user in the pair from validation set has read\n",
    "        b_user = bookUser_train[i[0]]\n",
    "        # users that have read the book in the pair in train data\n",
    "        if i[1] in userBook_train:\n",
    "            u_book = userBook_train[i[1]]\n",
    "        else:\n",
    "            u_book = []\n",
    "        # for every book that is not the book in the pair, get all users for each book \n",
    "        u_book_xb = []\n",
    "        for book in b_user:\n",
    "            if book in userBook_train:\n",
    "                u_book_xb += [userBook_train[book]]\n",
    "        scores = [0]\n",
    "        # compute\n",
    "        for user in u_book_xb:\n",
    "            scores.append(jaccard_similarity(user,u_book))\n",
    "        if np.mean(scores) > threshhold:\n",
    "            output_lst.append(1)\n",
    "        else:\n",
    "            output_lst.append(0)\n",
    "    return output_lst\n",
    "\n",
    "# create three jaccard similarity lists\n",
    "jaccard_lst1 = compute_jaccard_max(threshhold=0.01)\n",
    "jaccard_lst2 = compute_jaccard_max(threshhold=0.05)\n",
    "jaccard_lst3 = compute_jaccard_mean(threshhold=0.001)\n",
    "jaccard_lst4 = compute_jaccard_max_2(threshhold=0.03)\n",
    "#cosine_lst = compute_cosine_max(test_data, threshhold=0.028)\n",
    "\n",
    "#cosine_lst1=compute_cosine_max(test_data,threshhold=0.028)\n",
    "\n",
    "index = 0\n",
    "predictions = open(\"predictions_Read.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    t = 0\n",
    "    # first check if the book is in the popularity rank\n",
    "    if b in return4:\n",
    "        t = 1\n",
    "    # then check if the sum of three jaccard scores and the index is above 2\n",
    "    if t + jaccard_lst1[index] + jaccard_lst2[index] + jaccard_lst3[index] + jaccard_lst4[index] >= 3:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "    index += 1\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My accuracy on Kaggle is 0.66933, which has improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read the file and split it into training set and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For conviniences, I transform each element of the training and validation set into a dictionary from a string of dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "f = readGz('train_Category.json.gz')\n",
    "#header = f.readline()\n",
    "#header = header.strip().split('\\t')\n",
    "\n",
    "\n",
    "train_data = []\n",
    "valid_data = []\n",
    "data = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for line in f:\n",
    "    data.append(line)\n",
    "    if index < 190000:\n",
    "        train_data.append(line)\n",
    "    else:\n",
    "        valid_data.append(line)\n",
    "    index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I computed the 10 most common words with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most recent words are:\n",
      "(1421430, 'the')\n",
      "(858918, 'and')\n",
      "(754129, 'a')\n",
      "(716863, 'to')\n",
      "(699878, 'i')\n",
      "(622766, 'of')\n",
      "(420605, 'is')\n",
      "(408519, 'in')\n",
      "(392647, 'it')\n",
      "(370479, 'this')\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train_data:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "print(\"10 most recent words are:\")\n",
    "for i in range(10):\n",
    "    print(counts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, I used the feature function from class code, and apply it onto training set to generate feature vector. Then, I trained a logistic regression on the training dataset, and then test its accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score from the Logistic Regression: 0.6435\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# feature function from class note\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "# generate training x and y\n",
    "X_train = [feature(d) for d in train_data]\n",
    "y_train = [d['genreID'] for d in train_data]\n",
    "\n",
    "# generate validation x and y\n",
    "X_valid = [feature(d) for d in valid_data]\n",
    "y_valid = [d['genreID'] for d in valid_data]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# calculate the accuracy\n",
    "print('Accuracy Score from the Logistic Regression:', metrics.accuracy_score(clf.predict(X_valid), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I change the dictionary size to 2000 to increase my accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_8 = [x[1] for x in counts[:2000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I go through the same process as question 7 on the new word dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis\n",
    "wordId_8 = dict(zip(words_8, range(len(words_8))))\n",
    "wordSet_8 = set(words_8)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def feature_8(datum):\n",
    "    feat = [0]*len(words_8)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words_8:\n",
    "            feat[wordId_8[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X_train_8 = [feature_8(d) for d in train_data]\n",
    "y_train_8 = [d['genreID'] for d in train_data]\n",
    "\n",
    "clf_8 = LogisticRegression()\n",
    "clf_8.fit(X_train_8,y_train_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I got the new logistic model, I fit it on the testing set and then save it to the pairs_Category.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# read testing data set\n",
    "f = readGz('test_Category.json.gz')\n",
    "# convert the testing set in a dataframe for further use\n",
    "X_test = pd.DataFrame(f)\n",
    "# generate x from testing set\n",
    "X_test = X_test.apply(feature_8,axis = 1)\n",
    "X_test = pd.DataFrame.from_items(zip(X_test.index, X_test.values)).T\n",
    "data = pd.read_csv('pairs_Category.txt', sep=\",\", header=0)\n",
    "# use the trained model on the testing set\n",
    "data['prediction'] = clf_8.predict(X_test)\n",
    "# save the result to pairs_Category.txt\n",
    "data.to_csv('pairs_Category.txt', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My accuracy on Kaggle is 0.70566, which has improved and identical with the solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
